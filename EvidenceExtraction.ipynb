{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EvidenceExtraction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this project is to extract the evidences from the news articles and corresponding highlights.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Y7sicMRz4rHs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDxY_brj4b0R",
        "outputId": "8c2f2b8a-9fdd-419b-93e4-57bde9cfde8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets==1.1.2\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "Bq5hd6MSvfeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import transformers\n",
        "import os\n",
        "import tqdm\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import json"
      ],
      "metadata": {
        "id": "LZ9NE1pmvxKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e99e35f0-85e1-4ff7-ccda-2ae20df40289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")\n",
        "val_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"validation[:10%]\")"
      ],
      "metadata": {
        "id": "FV4FEIaHwItJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data))\n",
        "print(len(val_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAMgaLq4yMYV",
        "outputId": "3028c4b0-3501-4b26-a438-a5f47922a10a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "287113\n",
            "1337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_id_list = list(train_data['id'])\n",
        "train_article_list = list(train_data['article'])\n",
        "train_highlist_list = list(train_data['highlights'])\n",
        "\n",
        "val_id_list = list(val_data['id'])\n",
        "val_article_list = list(val_data['article'])\n",
        "val_highlist_list = list(val_data['highlights'])"
      ],
      "metadata": {
        "id": "UXPzdX3MN5jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "special_characters = ['!','@','#','$','%','^','&','*','(',')','-','+','?','_','=',',','<','>','/','.']\n",
        "\n",
        "train_data = []\n",
        "for i in tqdm(range(len(train_id_list))):\n",
        "  src = nltk.word_tokenize(train_article_list[i])\n",
        "  tgt = nltk.word_tokenize(train_highlist_list[i])\n",
        "  evdnc = []\n",
        "  text = ''\n",
        "  for w in tgt:\n",
        "    if w in src and w not in evdnc and w not in stop_words and w not in special_characters and len(w) >3 and not w.isnumeric():\n",
        "      evdnc.append(w)\n",
        "      text = text  + w.lower() + ' '\n",
        "  train_data.append({'src':train_article_list[i],'evd':text})\n",
        "\n",
        "val_data = []\n",
        "for i in tqdm(range(len(val_id_list))):\n",
        "  src = nltk.word_tokenize(val_article_list[i])\n",
        "  tgt = nltk.word_tokenize(val_highlist_list[i])\n",
        "  evdnc = []\n",
        "  text = ''\n",
        "  for w in tgt:\n",
        "    if w in src and w not in evdnc and w not in stop_words and w not in special_characters and len(w) >3 and not w.isnumeric():\n",
        "      evdnc.append(w)\n",
        "      text = text  + w.lower() + ' '\n",
        "  val_data.append({'src':val_article_list[i],'evd':text})\n",
        "\n",
        "with open('/content/gdrive/MyDrive/EvidenceQuery/train.json', 'w') as fp:\n",
        "  json.dump(train_data, fp)\n",
        "\n",
        "with open('/content/gdrive/MyDrive/EvidenceQuery/val.json', 'w') as fp:\n",
        "  json.dump(val_data, fp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABiGVgX2Ot71",
        "outputId": "9a7e389a-2744-4b36-871f-9d6223060cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 287113/287113 [34:40<00:00, 138.02it/s]\n",
            "100%|██████████| 1337/1337 [00:08<00:00, 154.18it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('/content/gdrive/MyDrive/EvidenceQuery/val.json')\n",
        " \n",
        "data = json.load(f)\n",
        " \n",
        "print (data)\n",
        "\n",
        "f.close()"
      ],
      "metadata": {
        "id": "yNe0-OIAhN4R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}